{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('gym': conda)",
   "metadata": {
    "interpreter": {
     "hash": "aa8547e2c612b9c9ea46f55e508d2f3aadcdb6ac7d52a0e5814b2fbbdfae8a9d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 7 DQN Extensions\n",
    "\n",
    "## Categorical DQN\n",
    "\n",
    "* [source code](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter07/07_dqn_distrib.py)\n",
    "* [Bellemare et al. 2017 Paper](https://arxiv.org/pdf/1707.06887v1.pdf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### key idea\n",
    "* Q value를  scalar value(기대값 형태의)가 아니라, 말그대로 가능한 Q value의 probability distribution으로 구하자는 아이디어.\n",
    "* 참고 할 만한 리뷰 논문은 [Lowet et al. 2020](https://www.sciencedirect.com/science/article/pii/S0166223620301983)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_STATES_IMG = False\n",
    "SAVE_TRANSTIONS_IMG = False\n",
    "\n",
    "if SAVE_STATES_IMG or SAVE_TRANSTIONS_IMG: # 그래프를 위한 code인듯\n",
    "    import matplotlib as mpl\n",
    "    mpl.use(\"Agg\")\n",
    "    import matplotlib.pylab as plt # pyplot이 아님."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51 # distribution의 bins number\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1) # size of bins\n",
    "\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DistributionalDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape) # conv가 아니라 input_shape이 들어가는 이유는 _get_conv_out function을 보면 이해 됨.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * N_ATOMS)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"supports\", torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z))\n"
   ]
  }
 ]
}