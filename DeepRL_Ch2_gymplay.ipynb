{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL hands-on by Maxim Lapan\n",
    "* conda activate gym \n",
    "  - which will work with torch 1.1, tensorflow 2.0 with CUDA 10\n",
    "* this book use torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02557767, -0.03932847,  0.04887002,  0.02317516])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = e.reset()\n",
    "obs # return 4 values, x coordinate, speed, angle, angular speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2) Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(e.action_space, e.observation_space)\n",
    "# action only left and right, space has 4 values with continue value [-inf, inf]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0247911 , -0.23511596,  0.04933352,  0.33086784]), 1.0, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0) # left action is taken, \n",
    "# new obs, reward, done flag deal with the end of episode.\n",
    "# extra information is {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample() # random action is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7583354e+00, -1.6064673e+38,  7.8152247e-02,  1.7623309e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7173146e+00,  5.9243443e+37,  1.4845206e-01, -1.5358119e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making randomly acting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    e = gym.make(\"CartPole-v0\")\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = e.reset()\n",
    "    # initialize the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 steps, 0 action, total reward 1.00 [ 0.00797062 -0.23835427  0.04959519  0.35409005]\n",
      "2 steps, 0 action, total reward 2.00 [ 0.00320353 -0.43414505  0.05667699  0.66199011]\n",
      "3 steps, 1 action, total reward 3.00 [-0.00547937 -0.23985559  0.06991679  0.387678  ]\n",
      "4 steps, 1 action, total reward 4.00 [-0.01027648 -0.04579214  0.07767035  0.11783281]\n",
      "5 steps, 1 action, total reward 5.00 [-0.01119232  0.148136    0.08002701 -0.14937027]\n",
      "6 steps, 1 action, total reward 6.00 [-0.0082296   0.34202622  0.0770396  -0.41577196]\n",
      "7 steps, 0 action, total reward 7.00 [-0.00138908  0.14590176  0.06872416 -0.0998306 ]\n",
      "8 steps, 1 action, total reward 8.00 [ 0.00152896  0.33997491  0.06672755 -0.37006414]\n",
      "9 steps, 1 action, total reward 9.00 [ 0.00832846  0.53408849  0.05932627 -0.64098317]\n",
      "10 steps, 1 action, total reward 10.00 [ 0.01901023  0.72833541  0.04650661 -0.91440926]\n",
      "11 steps, 1 action, total reward 11.00 [ 0.03357693  0.92279852  0.02821842 -1.19212052]\n",
      "12 steps, 1 action, total reward 12.00 [ 0.0520329   1.11754377  0.00437601 -1.47582698]\n",
      "13 steps, 1 action, total reward 13.00 [ 0.07438378  1.31261198 -0.02514053 -1.76713996]\n",
      "14 steps, 0 action, total reward 14.00 [ 0.10063602  1.11778286 -0.06048333 -1.48237916]\n",
      "15 steps, 0 action, total reward 15.00 [ 0.12299168  0.92344858 -0.09013091 -1.20918203]\n",
      "16 steps, 0 action, total reward 16.00 [ 0.14146065  0.72959877 -0.11431455 -0.9460502 ]\n",
      "17 steps, 0 action, total reward 17.00 [ 0.15605262  0.53618654 -0.13323556 -0.69136073]\n",
      "18 steps, 0 action, total reward 18.00 [ 0.16677635  0.34314017 -0.14706277 -0.44341302]\n",
      "19 steps, 0 action, total reward 19.00 [ 0.17363916  0.15037179 -0.15593103 -0.20046241]\n",
      "20 steps, 0 action, total reward 20.00 [ 0.17664659 -0.04221618 -0.15994028  0.03925647]\n",
      "21 steps, 0 action, total reward 21.00 [ 0.17580227 -0.23472623 -0.15915515  0.27751377]\n",
      "22 steps, 0 action, total reward 22.00 [ 0.17110775 -0.42726207 -0.15360487  0.51607317]\n",
      "23 steps, 1 action, total reward 23.00 [ 0.1625625  -0.23034874 -0.14328341  0.17919841]\n",
      "24 steps, 1 action, total reward 24.00 [ 0.15795553 -0.03349807 -0.13969944 -0.15503308]\n",
      "25 steps, 0 action, total reward 25.00 [ 0.15728557 -0.22637212 -0.1428001   0.09052133]\n",
      "26 steps, 0 action, total reward 26.00 [ 0.15275813 -0.41918922 -0.14098968  0.33496237]\n",
      "27 steps, 0 action, total reward 27.00 [ 0.14437434 -0.61205258 -0.13429043  0.58007308]\n",
      "28 steps, 0 action, total reward 28.00 [ 0.13213329 -0.80506248 -0.12268897  0.82761847]\n",
      "29 steps, 0 action, total reward 29.00 [ 0.11603204 -0.99831249 -0.1061366   1.07933442]\n",
      "30 steps, 0 action, total reward 30.00 [ 0.09606579 -1.19188514 -0.08454991  1.33691387]\n",
      "31 steps, 0 action, total reward 31.00 [ 0.07222809 -1.38584634 -0.05781163  1.60198829]\n",
      "32 steps, 0 action, total reward 32.00 [ 0.04451116 -1.58023821 -0.02577187  1.87610181]\n",
      "33 steps, 1 action, total reward 33.00 [ 0.0129064  -1.3848448   0.01175017  1.57553294]\n",
      "34 steps, 0 action, total reward 34.00 [-0.0147905  -1.58010482  0.04326083  1.87185722]\n",
      "35 steps, 1 action, total reward 35.00 [-0.0463926  -1.38548132  0.08069797  1.59290982]\n",
      "36 steps, 1 action, total reward 36.00 [-0.07410222 -1.1914044   0.11255617  1.32644129]\n",
      "37 steps, 1 action, total reward 37.00 [-0.09793031 -0.99786893  0.13908499  1.07099673]\n",
      "38 steps, 0 action, total reward 38.00 [-0.11788769 -1.19452822  0.16050493  1.40389631]\n",
      "39 steps, 1 action, total reward 39.00 [-0.14177825 -1.00172184  0.18858285  1.16539014]\n",
      "40 steps, 0 action, total reward 40.00 [-0.16181269 -1.19872968  0.21189066  1.51077805]\n",
      "Episode done in 40 steps, total reward 40.00\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action = e.action_space.sample() # random action\n",
    "    obs, reward, done, _ = e.step(action) # return the results\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    print(\"%d steps, %d action, total reward %.2f\" %(total_steps, action, total_reward), obs)\n",
    "    if done: # if done flag return True, end episode.\n",
    "        break\n",
    "print(\"Episode done in %d steps, total reward %.2f\" %(total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "    def action(self, action):\n",
    "        if random.random() < self.epsilon: # 0.1 probability make random action.\n",
    "            print(\"Random!\")\n",
    "            return self.env.action_space.sample()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 steps,total reward 1.00 [-0.02835297 -0.17819333  0.00204605  0.31641757]\n",
      "2 steps,total reward 2.00 [-0.03191684 -0.37334437  0.0083744   0.60974505]\n",
      "3 steps,total reward 3.00 [-0.03938373 -0.56858238  0.02056931  0.90505384]\n",
      "4 steps,total reward 4.00 [-0.05075538 -0.76397675  0.03867038  1.20413035]\n",
      "5 steps,total reward 5.00 [-0.06603491 -0.95957666  0.06275299  1.50867713]\n",
      "6 steps,total reward 6.00 [-0.08522644 -1.15540061  0.09292653  1.82027216]\n",
      "7 steps,total reward 7.00 [-0.10833446 -1.35142415  0.12933197  2.14031992]\n",
      "8 steps,total reward 8.00 [-0.13536294 -1.54756516  0.17213837  2.46999196]\n",
      "9 steps,total reward 9.00 [-0.16631424 -1.74366683  0.22153821  2.81015622]\n",
      "Reward got: 9.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v0\")) # env is wrapped by ActionWrapper.\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0) # action will be 0, otherwise random action took with 0.1 probability.\n",
    "# question, how do I know the taken action in the wrapper?\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        print(\"%d steps,total reward %.2f\" %(total_steps, total_reward), obs)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Reward got: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Trying to write to monitor directory recording0 with existing monitor files: recording0/openaigym.manifest.1.14711.manifest.json.\n\n You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-919c0d37bad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recording0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# directory name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# it need FFmpeg to make mp4 file, also you need X11 session, forwarding for ssh.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, directory, video_callable, force, resume, write_upon_reset, uid, mode)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         self._start(directory, video_callable, force, resume,\n\u001b[0;32m---> 27\u001b[0;31m                             write_upon_reset, uid, mode)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, directory, video_callable, force, resume, write_upon_reset, uid, mode)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 raise error.Error('''Trying to write to monitor directory {} with existing monitor files: {}.\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m  You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.'''.format(directory, ', '.join(training_manifests[:5])))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_closer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Trying to write to monitor directory recording0 with existing monitor files: recording0/openaigym.manifest.1.14711.manifest.json.\n\n You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording0\") # directory name.\n",
    "# it need FFmpeg to make mp4 file, also you need X11 session, forwarding for ssh.\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "    env.close()\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
