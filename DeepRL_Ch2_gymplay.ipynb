{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL hands-on by Maxim Lapan\n",
    "* conda activate gym \n",
    "  - which will work with torch 1.1, tensorflow 2.0 with CUDA 10\n",
    "* this book use torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00602764, -0.04494352, -0.01977204, -0.00313485])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = e.reset()\n",
    "obs # return 4 values, x coordinate, speed, angle, angular speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2) Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(e.action_space, e.observation_space)\n",
    "# action only left and right, space has 4 values with continue value [-inf, inf]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00692651, -0.23977641, -0.01983474,  0.28324477]), 1.0, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0) # left action is taken, \n",
    "# new obs, reward, done flag deal with the end of episode.\n",
    "# extra information is {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample() # random action is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9349544e+00,  1.9150158e+38, -2.2192951e-01,  2.5455205e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.4707634e-02, 3.1531373e+38, 3.2359022e-01, 2.5518052e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making randomly acting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    e = gym.make(\"CartPole-v0\")\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = e.reset()\n",
    "    # initialize the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 steps, 0 action, total reward 1.00 [ 0.03989026 -0.18203008  0.01474504  0.30295928]\n",
      "2 steps, 1 action, total reward 2.00 [0.03624966 0.01287866 0.02080423 0.01496284]\n",
      "3 steps, 0 action, total reward 3.00 [ 0.03650723 -0.18253537  0.02110349  0.31413647]\n",
      "4 steps, 0 action, total reward 4.00 [ 0.03285652 -0.37795149  0.02738622  0.61339934]\n",
      "5 steps, 0 action, total reward 5.00 [ 0.02529749 -0.57344523  0.0396542   0.91458053]\n",
      "6 steps, 1 action, total reward 6.00 [ 0.01382859 -0.3788814   0.05794581  0.6346195 ]\n",
      "7 steps, 1 action, total reward 7.00 [ 0.00625096 -0.18461354  0.0706382   0.36073333]\n",
      "8 steps, 0 action, total reward 8.00 [ 0.00255869 -0.38066476  0.07785287  0.67482714]\n",
      "9 steps, 0 action, total reward 9.00 [-0.00505461 -0.57677738  0.09134941  0.99097065]\n",
      "10 steps, 1 action, total reward 10.00 [-0.01659016 -0.38298883  0.11116883  0.72831922]\n",
      "11 steps, 1 action, total reward 11.00 [-0.02424993 -0.18956488  0.12573521  0.47259062]\n",
      "12 steps, 0 action, total reward 12.00 [-0.02804123 -0.3862175   0.13518702  0.80210974]\n",
      "13 steps, 0 action, total reward 13.00 [-0.03576558 -0.58290867  0.15122922  1.13407866]\n",
      "14 steps, 1 action, total reward 14.00 [-0.04742375 -0.3900539   0.17391079  0.89239029]\n",
      "15 steps, 1 action, total reward 15.00 [-0.05522483 -0.19766315  0.1917586   0.65902975]\n",
      "16 steps, 1 action, total reward 16.00 [-0.05917809 -0.00565394  0.20493919  0.4323272 ]\n",
      "17 steps, 1 action, total reward 17.00 [-0.05929117  0.18606662  0.21358574  0.21059571]\n",
      "Episode done in 17 steps, total reward 17.00\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action = e.action_space.sample() # random action\n",
    "    obs, reward, done, _ = e.step(action) # return the results\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    print(\"%d steps, %d action, total reward %.2f\" %(total_steps, action, total_reward), obs)\n",
    "    if done: # if done flag return True, end episode.\n",
    "        break\n",
    "print(\"Episode done in %d steps, total reward %.2f\" %(total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "    def action(self, action):\n",
    "        if random.random() < self.epsilon: # 0.1 probability make random action.\n",
    "            print(\"Random!\")\n",
    "            return self.env.action_space.sample()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 steps,total reward 1.00 [-0.00288824 -0.20683474 -0.04462909  0.28648165]\n",
      "2 steps,total reward 2.00 [-0.00702494 -0.40129275 -0.03889946  0.56476162]\n",
      "3 steps,total reward 3.00 [-0.01505079 -0.59584796 -0.02760423  0.84494011]\n",
      "Random!\n",
      "4 steps,total reward 4.00 [-0.02696775 -0.40036044 -0.01070543  0.54370594]\n",
      "5 steps,total reward 5.00 [-3.49749619e-02 -5.95330329e-01  1.68692215e-04  8.32996670e-01]\n",
      "6 steps,total reward 6.00 [-0.04688157 -0.79045458  0.01682863  1.12573264]\n",
      "7 steps,total reward 7.00 [-0.06269066 -0.98579298  0.03934328  1.42364614]\n",
      "8 steps,total reward 8.00 [-0.08240652 -1.18137868  0.0678162   1.72836159]\n",
      "9 steps,total reward 9.00 [-0.10603409 -1.37720695  0.10238343  2.04135147]\n",
      "10 steps,total reward 10.00 [-0.13357823 -1.57322166  0.14321046  2.36388202]\n",
      "11 steps,total reward 11.00 [-0.16504267 -1.76929918  0.1904881   2.69694751]\n",
      "12 steps,total reward 12.00 [-0.20042865 -1.96522989  0.24442705  3.04119299]\n",
      "Reward got: 12.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v0\")) # env is wrapped by ActionWrapper.\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0) # action will be 0, otherwise random action took with 0.1 probability.\n",
    "# question, how do I know the taken action in the wrapper?\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        print(\"%d steps,total reward %.2f\" %(total_steps, total_reward), obs)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Reward got: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 22 steps, total reward 22.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording0\") # directory name.\n",
    "# it need FFmpeg to make mp4 file, also you need X11 session, forwarding for ssh.\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "    env.close()\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
