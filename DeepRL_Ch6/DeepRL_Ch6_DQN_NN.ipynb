{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL hands-on by Maxim Lapan\n",
    "## Chapter 6 Deep Q-Networks\n",
    "\n",
    "* conda activate gym\n",
    "* [source github](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)\n",
    "* from this part, it need modularized code. Some kind of code woudl be packed in lib directory. Be aware that\n",
    "\n",
    "## DQN for Pong"
   ]
  },
  {
   "source": [
    "## Q-learning algorithm 복습\n",
    "\n",
    "1. Q(s,a)를 초기화한다.(Q는 action까지 특정한 가치함수임)\n",
    "2. 환경과 상호작용하여, tuple(s,a,r,s')를 얻는다.\n",
    "3. 손실값을 계산한다. \n",
    "    1. 에피소드가 종료될땐 L = (Q(s,a) - r)^2\n",
    "    2. 그렇지 않다면 L = (Q(s,a) -(r + gamma * max(Q(s',a'), a' in A))^2\n",
    "4. Q(s,a)를 갱신하는데, stochastic gradient descent (SGD) algorithm를 사용한다. 이 때 경사하강 방향은 Loss를 최소화하는 방향이다.\n",
    "5. 수렴할 때까지 2번 단계부터 반복한다.\n",
    "\n",
    "### epsilon-greedy method\n",
    "* exploitation vs exploration 문제를 해결하기 위한 가장 단순한 방법.\n",
    "* hyperparameter epsilon를 설정해두고(보통 p 확률 값으로 정한다), 이 값 이하면 random action을 선택 - exploration. 그 이상이면 policy (Q learning의 경우 best Q)를 따른다 - exploitation.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DQN에서 고려해야 할 요소들\n",
    "\n",
    "### SGD optimization\n",
    "* 손실함수를 최소화하는 방식은 supervised learning의 기본이다. 여기에는 몇가지 전제가 깔려 했는데, 가장 중요한 것은 data의 분표가 i.i.d를 따른다는 것이다. independent and identically distributed. 이는 SGD를 사용할 때 중요한 조건이다. (사실 supervised learning에 쓰이는 data set은 i.i.d를 따를지 몰라도, 현실 세계의 사물은 그렇게 i.i.d하진 않다.)\n",
    "* RL에서는 당연히 i.i.d를 따르지 않는다.\n",
    "    * env로부터 들어오는 sample은 independent하지 않다. \n",
    "    * data를 batch화 하더라도, independent하지 않고, identical하지도 않는다. episode 의존적이고, 이때 내재된 policy에 의존적이기 때문에 identical하지 않은 것이다. 만약 policy가 random이라면 identical할수도 있겠지만, 우리가 원하는 것은 random policy가 아니다. {Successor representation learning에서는 가치와 상태이전함수를 분리했고, 상태이전함수만을 학습하기 위해서는 random policy를 쓰기도 한다.}\n",
    "* 이 문제를 해결하기 위해서 고안된 것이 replay buffer다. 의존성을 띄는 최근 sample data가 아니라, 훨씬 과거에 저장된 sample을 이용해 SGD를 적용하는 것이다. 하지만, policy가 update되면 지나치가 과거의 sample은 도움되지 않기 때문에 차츰 밀어낸다. 그래서 buffer라고 불리는 것. {이런 고안 배경을 고려하면 replay buffer를 hippocampal memory에 비유하는 것은 적절하지 않아 보인다. 어떤 net을 최적화를 위해 사용되는 buffer와 imagination이나 planning을 위해 사용되는 memory를 좀 다른 듯하다. 물론 cortex에 있는 neural circuit을 최적화하기 위해서 hippocampal memory를 사용한다면 가능한 얘기지만, 과연 그런가? }\n",
    "\n",
    "### Correlation between steps\n",
    "* Q(s,a)가 Q(s',a')를 통해 update될 때 tabular method에서는 크게 문제 없지만, neural net에서 보면 s와 s'는 크게 다르지 않다. 그래서 Q(s,a)를 update하면 Q(s',a')도 update될 수 있다. 이러면 학습이 unstable해진다. \n",
    "* 그래서 target network를 따로 만들었다. 이건 원래 network의 copy version으로, 여기서 Q(s,a) update용 Q(s',a')값을 가져온다. 그리고 두 개의 net는 주기적으로만 동기화 시킨다. (step으로 봤을때, 1k or 10k iterations 정도)\n",
    "\n",
    "### The Markov property\n",
    "* Markov 환경이라면 전체 환경이 관찰되어야 하지만, atari game을 포함해 대부분의 현실 문제는 그렇지 않다. 이걸 partially observable MDPs(POMDP) 문제라 한다. pong의 경우 한 번에 하나의 frame만 관찰 가능하다. \n",
    "* atari game 같은 문제를 풀기 위해서 보통 4개의 frame을 stack으로 처리해 훈련시켜서 POMDP를 약간 극복하려 한다. \n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.bool), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "# !pip install gym[atari]\n",
    "env = wrappers.make_env(DEFAULT_ENV_NAME) # \"PongNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DQN(\n  (conv): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=3136, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=6, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pong env.observation space - (4, 84, 84)\n",
    "action space - 6\n",
    "'''\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'GeForce GTX 1080'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DQN(\n  (conv): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=3136, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=6, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(tgt_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_mean_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "898: done 1 games, mean reward -20.000, eps 0.99, speed 942.30 f/s\n",
      "2056: done 2 games, mean reward -19.000, eps 0.98, speed 709.29 f/s\n",
      "Best mean reward updated -20.000 -> -19.000, model saved\n",
      "2925: done 3 games, mean reward -19.667, eps 0.97, speed 997.27 f/s\n",
      "3988: done 4 games, mean reward -19.750, eps 0.96, speed 999.06 f/s\n",
      "4830: done 5 games, mean reward -20.000, eps 0.95, speed 989.96 f/s\n",
      "5812: done 6 games, mean reward -20.000, eps 0.94, speed 990.58 f/s\n",
      "6622: done 7 games, mean reward -20.143, eps 0.93, speed 987.01 f/s\n",
      "7446: done 8 games, mean reward -20.250, eps 0.93, speed 985.68 f/s\n",
      "8358: done 9 games, mean reward -20.333, eps 0.92, speed 985.98 f/s\n",
      "9120: done 10 games, mean reward -20.400, eps 0.91, speed 962.95 f/s\n",
      "9910: done 11 games, mean reward -20.455, eps 0.90, speed 964.69 f/s\n",
      "10691: done 12 games, mean reward -20.500, eps 0.89, speed 136.60 f/s\n",
      "11674: done 13 games, mean reward -20.462, eps 0.88, speed 125.45 f/s\n",
      "12634: done 14 games, mean reward -20.500, eps 0.87, speed 130.48 f/s\n",
      "13396: done 15 games, mean reward -20.533, eps 0.87, speed 124.79 f/s\n",
      "14254: done 16 games, mean reward -20.562, eps 0.86, speed 130.74 f/s\n",
      "15076: done 17 games, mean reward -20.588, eps 0.85, speed 128.21 f/s\n",
      "15978: done 18 games, mean reward -20.556, eps 0.84, speed 125.25 f/s\n",
      "16948: done 19 games, mean reward -20.579, eps 0.83, speed 129.10 f/s\n",
      "18056: done 20 games, mean reward -20.450, eps 0.82, speed 126.18 f/s\n",
      "19061: done 21 games, mean reward -20.429, eps 0.81, speed 128.95 f/s\n",
      "19971: done 22 games, mean reward -20.455, eps 0.80, speed 129.43 f/s\n",
      "20941: done 23 games, mean reward -20.478, eps 0.79, speed 129.97 f/s\n",
      "21809: done 24 games, mean reward -20.500, eps 0.78, speed 130.62 f/s\n",
      "22696: done 25 games, mean reward -20.480, eps 0.77, speed 125.03 f/s\n",
      "23638: done 26 games, mean reward -20.500, eps 0.76, speed 122.04 f/s\n",
      "24539: done 27 games, mean reward -20.519, eps 0.75, speed 126.77 f/s\n",
      "25757: done 28 games, mean reward -20.429, eps 0.74, speed 123.89 f/s\n",
      "26715: done 29 games, mean reward -20.414, eps 0.73, speed 99.14 f/s\n",
      "27712: done 30 games, mean reward -20.367, eps 0.72, speed 114.42 f/s\n",
      "28534: done 31 games, mean reward -20.387, eps 0.71, speed 108.60 f/s\n",
      "29905: done 32 games, mean reward -20.344, eps 0.70, speed 107.58 f/s\n",
      "30834: done 33 games, mean reward -20.333, eps 0.69, speed 114.34 f/s\n",
      "31686: done 34 games, mean reward -20.353, eps 0.68, speed 110.33 f/s\n",
      "32716: done 35 games, mean reward -20.286, eps 0.67, speed 110.16 f/s\n",
      "33888: done 36 games, mean reward -20.278, eps 0.66, speed 114.45 f/s\n",
      "34931: done 37 games, mean reward -20.297, eps 0.65, speed 116.00 f/s\n",
      "35771: done 38 games, mean reward -20.289, eps 0.64, speed 112.52 f/s\n",
      "36641: done 39 games, mean reward -20.308, eps 0.63, speed 111.06 f/s\n",
      "37755: done 40 games, mean reward -20.325, eps 0.62, speed 115.77 f/s\n",
      "38733: done 41 games, mean reward -20.317, eps 0.61, speed 121.71 f/s\n",
      "39668: done 42 games, mean reward -20.286, eps 0.60, speed 124.21 f/s\n",
      "40449: done 43 games, mean reward -20.302, eps 0.60, speed 121.51 f/s\n",
      "41640: done 44 games, mean reward -20.227, eps 0.58, speed 126.25 f/s\n",
      "42553: done 45 games, mean reward -20.244, eps 0.57, speed 124.13 f/s\n",
      "43933: done 46 games, mean reward -20.239, eps 0.56, speed 121.62 f/s\n",
      "45357: done 47 games, mean reward -20.234, eps 0.55, speed 123.36 f/s\n",
      "46427: done 48 games, mean reward -20.229, eps 0.54, speed 118.49 f/s\n",
      "47479: done 49 games, mean reward -20.224, eps 0.53, speed 122.69 f/s\n",
      "48733: done 50 games, mean reward -20.180, eps 0.51, speed 125.87 f/s\n",
      "50174: done 51 games, mean reward -20.176, eps 0.50, speed 125.86 f/s\n",
      "51228: done 52 games, mean reward -20.173, eps 0.49, speed 122.86 f/s\n",
      "52020: done 53 games, mean reward -20.189, eps 0.48, speed 124.52 f/s\n",
      "53158: done 54 games, mean reward -20.204, eps 0.47, speed 121.25 f/s\n",
      "54672: done 55 games, mean reward -20.182, eps 0.45, speed 121.42 f/s\n",
      "56152: done 56 games, mean reward -20.143, eps 0.44, speed 122.91 f/s\n",
      "57384: done 57 games, mean reward -20.140, eps 0.43, speed 124.84 f/s\n",
      "58538: done 58 games, mean reward -20.138, eps 0.41, speed 124.14 f/s\n",
      "59907: done 59 games, mean reward -20.102, eps 0.40, speed 123.29 f/s\n",
      "61470: done 60 games, mean reward -20.083, eps 0.39, speed 122.34 f/s\n",
      "62823: done 61 games, mean reward -20.098, eps 0.37, speed 118.50 f/s\n",
      "64099: done 62 games, mean reward -20.065, eps 0.36, speed 122.33 f/s\n",
      "65582: done 63 games, mean reward -20.048, eps 0.34, speed 121.37 f/s\n",
      "67457: done 64 games, mean reward -20.016, eps 0.33, speed 121.89 f/s\n",
      "69387: done 65 games, mean reward -19.954, eps 0.31, speed 123.20 f/s\n",
      "70533: done 66 games, mean reward -19.970, eps 0.29, speed 120.51 f/s\n",
      "72655: done 67 games, mean reward -19.836, eps 0.27, speed 120.97 f/s\n",
      "74303: done 68 games, mean reward -19.809, eps 0.26, speed 120.55 f/s\n",
      "76319: done 69 games, mean reward -19.725, eps 0.24, speed 122.13 f/s\n",
      "78103: done 70 games, mean reward -19.686, eps 0.22, speed 117.50 f/s\n",
      "79813: done 71 games, mean reward -19.648, eps 0.20, speed 121.62 f/s\n",
      "81567: done 72 games, mean reward -19.625, eps 0.18, speed 116.82 f/s\n",
      "83465: done 73 games, mean reward -19.603, eps 0.17, speed 120.09 f/s\n",
      "84773: done 74 games, mean reward -19.622, eps 0.15, speed 118.92 f/s\n",
      "86240: done 75 games, mean reward -19.613, eps 0.14, speed 119.02 f/s\n",
      "87560: done 76 games, mean reward -19.605, eps 0.12, speed 114.19 f/s\n",
      "89361: done 77 games, mean reward -19.545, eps 0.11, speed 115.55 f/s\n",
      "91301: done 78 games, mean reward -19.526, eps 0.09, speed 119.60 f/s\n",
      "93292: done 79 games, mean reward -19.506, eps 0.07, speed 120.30 f/s\n",
      "95343: done 80 games, mean reward -19.488, eps 0.05, speed 119.69 f/s\n",
      "96945: done 81 games, mean reward -19.494, eps 0.03, speed 120.85 f/s\n",
      "98963: done 82 games, mean reward -19.463, eps 0.02, speed 119.09 f/s\n",
      "100871: done 83 games, mean reward -19.434, eps 0.02, speed 120.34 f/s\n",
      "103136: done 84 games, mean reward -19.369, eps 0.02, speed 118.52 f/s\n",
      "105155: done 85 games, mean reward -19.329, eps 0.02, speed 115.20 f/s\n",
      "107666: done 86 games, mean reward -19.256, eps 0.02, speed 116.30 f/s\n",
      "110132: done 87 games, mean reward -19.195, eps 0.02, speed 115.39 f/s\n",
      "113313: done 88 games, mean reward -19.045, eps 0.02, speed 117.33 f/s\n",
      "116245: done 89 games, mean reward -18.955, eps 0.02, speed 119.34 f/s\n",
      "Best mean reward updated -19.000 -> -18.955, model saved\n",
      "119197: done 90 games, mean reward -18.889, eps 0.02, speed 110.65 f/s\n",
      "Best mean reward updated -18.955 -> -18.889, model saved\n",
      "122230: done 91 games, mean reward -18.780, eps 0.02, speed 118.68 f/s\n",
      "Best mean reward updated -18.889 -> -18.780, model saved\n",
      "125053: done 92 games, mean reward -18.685, eps 0.02, speed 120.99 f/s\n",
      "Best mean reward updated -18.780 -> -18.685, model saved\n",
      "128375: done 93 games, mean reward -18.559, eps 0.02, speed 120.84 f/s\n",
      "Best mean reward updated -18.685 -> -18.559, model saved\n",
      "131112: done 94 games, mean reward -18.489, eps 0.02, speed 120.91 f/s\n",
      "Best mean reward updated -18.559 -> -18.489, model saved\n",
      "134515: done 95 games, mean reward -18.368, eps 0.02, speed 120.49 f/s\n",
      "Best mean reward updated -18.489 -> -18.368, model saved\n",
      "137221: done 96 games, mean reward -18.312, eps 0.02, speed 120.22 f/s\n",
      "Best mean reward updated -18.368 -> -18.312, model saved\n",
      "139285: done 97 games, mean reward -18.309, eps 0.02, speed 120.38 f/s\n",
      "Best mean reward updated -18.312 -> -18.309, model saved\n",
      "141968: done 98 games, mean reward -18.224, eps 0.02, speed 120.46 f/s\n",
      "Best mean reward updated -18.309 -> -18.224, model saved\n",
      "145381: done 99 games, mean reward -18.101, eps 0.02, speed 120.24 f/s\n",
      "Best mean reward updated -18.224 -> -18.101, model saved\n",
      "148794: done 100 games, mean reward -18.020, eps 0.02, speed 120.56 f/s\n",
      "Best mean reward updated -18.101 -> -18.020, model saved\n",
      "151961: done 101 games, mean reward -17.920, eps 0.02, speed 120.39 f/s\n",
      "Best mean reward updated -18.020 -> -17.920, model saved\n",
      "155628: done 102 games, mean reward -17.750, eps 0.02, speed 119.57 f/s\n",
      "Best mean reward updated -17.920 -> -17.750, model saved\n",
      "158870: done 103 games, mean reward -17.600, eps 0.02, speed 118.90 f/s\n",
      "Best mean reward updated -17.750 -> -17.600, model saved\n",
      "161993: done 104 games, mean reward -17.480, eps 0.02, speed 117.39 f/s\n",
      "Best mean reward updated -17.600 -> -17.480, model saved\n",
      "165786: done 105 games, mean reward -17.310, eps 0.02, speed 117.94 f/s\n",
      "Best mean reward updated -17.480 -> -17.310, model saved\n",
      "168119: done 106 games, mean reward -17.190, eps 0.02, speed 118.54 f/s\n",
      "Best mean reward updated -17.310 -> -17.190, model saved\n",
      "170796: done 107 games, mean reward -17.040, eps 0.02, speed 119.77 f/s\n",
      "Best mean reward updated -17.190 -> -17.040, model saved\n",
      "173785: done 108 games, mean reward -16.900, eps 0.02, speed 119.44 f/s\n",
      "Best mean reward updated -17.040 -> -16.900, model saved\n",
      "177000: done 109 games, mean reward -16.730, eps 0.02, speed 120.49 f/s\n",
      "Best mean reward updated -16.900 -> -16.730, model saved\n",
      "179918: done 110 games, mean reward -16.610, eps 0.02, speed 119.94 f/s\n",
      "Best mean reward updated -16.730 -> -16.610, model saved\n",
      "182550: done 111 games, mean reward -16.510, eps 0.02, speed 120.25 f/s\n",
      "Best mean reward updated -16.610 -> -16.510, model saved\n",
      "185442: done 112 games, mean reward -16.350, eps 0.02, speed 120.65 f/s\n",
      "Best mean reward updated -16.510 -> -16.350, model saved\n",
      "189802: done 113 games, mean reward -16.130, eps 0.02, speed 120.26 f/s\n",
      "Best mean reward updated -16.350 -> -16.130, model saved\n",
      "193217: done 114 games, mean reward -15.880, eps 0.02, speed 120.28 f/s\n",
      "Best mean reward updated -16.130 -> -15.880, model saved\n",
      "196572: done 115 games, mean reward -15.650, eps 0.02, speed 119.83 f/s\n",
      "Best mean reward updated -15.880 -> -15.650, model saved\n",
      "199869: done 116 games, mean reward -15.400, eps 0.02, speed 111.63 f/s\n",
      "Best mean reward updated -15.650 -> -15.400, model saved\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6681c58474a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f456a966fdbc>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # it's convenient to record the date with current time\n",
    "LOGDIR = './tmp/ch6' + DEFAULT_ENV_NAME + '/' + current_time + '/' \n",
    "writer = tf.summary.create_file_writer(LOGDIR)\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        \n",
    "        with writer.as_default():    \n",
    "            tf.summary.scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            tf.summary.scalar(\"speed\", speed, frame_idx)\n",
    "            tf.summary.scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            tf.summary.scalar(\"reward\", reward, frame_idx)\n",
    "        \n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device = device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}